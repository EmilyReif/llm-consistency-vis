<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="/llm-consistency-vis/favicon.ico"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="description" content="Interactive visualization tool for exploring LLM output distributions and consistency patterns"/><meta name="keywords" content="LLM, language model, visualization, consistency, output distribution, AI research"/><meta name="author" content="Emily Reif, Deniz Nazarova, Jared Hwang, Claire Yang"/><meta property="og:type" content="website"/><meta property="og:title" content="LLM Consistency Visualization"/><meta property="og:description" content="Interactive tool for exploring language model output distributions and understanding consistency patterns"/><meta property="og:url" content="https://emilyreif.com/llm-consistency-vis/"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="LLM Consistency Visualization"/><meta property="twitter:description" content="Interactive tool for exploring language model output distributions and understanding consistency patterns"/><link rel="apple-touch-icon" href="/llm-consistency-vis/logo192.png"/><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"><link rel="manifest" href="/llm-consistency-vis/manifest.json"/><title>LLM Consistency Visualization - Explore Language Model Output Distributions</title><style>body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Oxygen,Ubuntu,Cantarell,sans-serif;margin:0 auto;padding:20px}.content-section{background-color:#d7f8d9;border-radius:15px;padding:20px;margin:20px 0}.authors{font-size:smaller;color:#666;font-style:italic}h1{margin-top:0}.content-section h2{color:#333;margin-top:0}header{font-family:monospace;font-size:12px;line-height:1.5em}footer{margin-top:40px;text-align:center}</style><script defer="defer" src="/llm-consistency-vis/static/js/main.c7f64ad6.js"></script><link href="/llm-consistency-vis/static/css/main.08361fc1.css" rel="stylesheet"></head><body><header><h1>Visualizing LLM outputs</h1><p class="authors">Emily Reif, Deniz Nazarova, Jared Hwang, Claire Yang</p><p>When an LLM returns a response, we’re actually sampling from a probability distribution over many possible outputs. But we usually only see one of those samples—the response that gets returned.</p><p>If we’re just using the model to get an answer or write some text, that’s fine. But if we want to understand how the model behaves—or build systems that depend on it—we need more than just one response. <b>We need to understand the whole distribution of possible outputs.</b></p></header><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div></body></html>
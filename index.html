<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="/llm-consistency-vis/favicon.ico"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="description" content="Web site created using create-react-app"/><link rel="apple-touch-icon" href="/llm-consistency-vis/logo192.png"/><link rel="manifest" href="/llm-consistency-vis/manifest.json"/><title>LLM Output Visualization</title><style>body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Oxygen,Ubuntu,Cantarell,sans-serif;max-width:1200px;margin:0 auto;padding:20px}.content-section{background-color:#d7f8d9;border-radius:15px;padding:20px;margin:20px 0}.authors{font-size:smaller;color:#666;font-style:italic}h1{margin-top:0}.content-section h2{color:#333;margin-top:0}header{font-family:monospace;font-size:12px;line-height:1.5em}footer{margin-top:40px;text-align:center}</style><script defer="defer" src="/llm-consistency-vis/static/js/main.a98119fd.js"></script><link href="/llm-consistency-vis/static/css/main.af7c462b.css" rel="stylesheet"></head><body><header><h1>Visualizing LLM outputs</h1><p class="authors">Emily Reif, Deniz Nazarova, Jared Hwang, Claire Yang</p><p>When an LLM returns a response, we’re actually sampling from a probability distribution over many possible outputs. But we usually only see one of those samples—the response that gets returned.</p><p>If we’re just using the model to get an answer or write some text, that’s fine. But if we want to understand how the model behaves—or build systems that depend on it—we need more than just one response. <b>We need to understand the whole distribution of possible outputs.</b></p></header><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div></body></html>
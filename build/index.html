<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="description" content="Web site created using create-react-app"/><link rel="apple-touch-icon" href="/logo192.png"/><link rel="manifest" href="/manifest.json"/><title>LLM Output Visualization</title><style>body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Oxygen,Ubuntu,Cantarell,sans-serif;max-width:1200px;margin:0 auto;padding:20px}.content-section{background-color:#d7f8d9;border-radius:15px;padding:20px;margin:20px 0}.authors{font-size:smaller;color:#666;font-style:italic}h1{margin-top:0}.content-section h2{color:#333;margin-top:0}header{font-family:monospace;font-size:12px;line-height:1.5em}footer{margin-top:40px;text-align:center}</style><script defer="defer" src="/static/js/main.6733678b.js"></script><link href="/static/css/main.af7c462b.css" rel="stylesheet"></head><body><header><h1>Visualizing LLM outputs</h1><p class="authors">Emily Reif, Deniz Nazarova, Jared Hwang, Claire Yang</p><p>This visualization explores diversity and consistency in Large Language Model (LLM) outputs by showing the distribution of responses to help users understand whether a single response represents pattern or an outlier.</p><p>We aim to help users optimize prompt engineering™️, evaluate whether an LLM is safe and reliable for their specific use cases through revealing systematic patterns in how models respond to different types of queries.</p></header><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div><footer><p><a href="https://gitlab.cs.washington.edu/cse512/25sp/fp/llm-outputs-vs-expectations">Repository</a><br><a href="https://courses.cs.washington.edu/courses/cse512/25sp/">CSE 512 Data Visualization</a><br><a href="https://www.washington.edu">University of Washington</a></p></footer></body></html>
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User Study Results Analysis\n",
        "\n",
        "This notebook analyzes the results from the LLM consistency visualization user study.\n",
        "\n",
        "**Data sources:**\n",
        "- **Comparisons**: Exit interview questions comparing graph vs list interfaces (1-7 scale: 1=graph, 7=list)\n",
        "- **Single Distribution**: Per-interface questionnaire responses (one for list+dataset A, one for graph+dataset B per participant)\n",
        "- **Outputs**: Telemetry (timing analysis; accuracy to be added)\n",
        "\n",
        "**Key filtering**: We only include participants who completed the Comparisons survey (have prolific_pid in metadata). All other data is filtered to these PIDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import seaborn as sns\n",
        "import re\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Seaborn styling for prettier plots (no grid background)\n",
        "sns.set_theme(style=\"white\", palette=\"muted\", font_scale=1.1)\n",
        "\n",
        "# Paths: works if run from repo root or from user_study_results/\n",
        "DATA_DIR = Path(\"user_study_results\")\n",
        "if not (DATA_DIR / \"user_study_logging - Comparison.csv\").exists():\n",
        "    DATA_DIR = Path(\".\")\n",
        "\n",
        "COMPARISONS_PATH = DATA_DIR / \"user_study_logging - Comparison.csv\"\n",
        "SINGLE_DIST_PATH = DATA_DIR / \"user_study_logging - Single Distribution.csv\"\n",
        "OUTPUTS_PATH = DATA_DIR / \"user_study_logging - outputs.csv\"\n",
        "\n",
        "comparisons_df = pd.read_csv(COMPARISONS_PATH)\n",
        "single_dist_df = pd.read_csv(SINGLE_DIST_PATH)\n",
        "outputs_df = pd.read_csv(OUTPUTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Extract Prolific IDs from Comparisons\n",
        "\n",
        "Parse the \"Metadata - no need to edit this\" column which contains URLs with `prolific_pid=...`.\n",
        "These PIDs define our analysis cohort—we filter ALL other data to only include these participants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_prolific_pids(metadata_series):\n",
        "    \"\"\"Extract prolific_pid values from metadata URL strings.\"\"\"\n",
        "    pids = set()\n",
        "    for val in metadata_series.dropna():\n",
        "        s = str(val)\n",
        "        match = re.search(r'prolific_pid=([a-f0-9]+)', s, re.IGNORECASE)\n",
        "        if match:\n",
        "            pids.add(match.group(1))\n",
        "    return pids\n",
        "\n",
        "# Get metadata column - handle possible variations in column name\n",
        "meta_col = [c for c in comparisons_df.columns if 'Metadata' in c and 'no need' in c.lower()]\n",
        "if not meta_col:\n",
        "    meta_col = [c for c in comparisons_df.columns if 'Metadata' in c]\n",
        "meta_col = meta_col[0] if meta_col else 'Metadata - no need to edit this'\n",
        "\n",
        "valid_prolific_pids = extract_prolific_pids(comparisons_df[meta_col])\n",
        "print(f\"Found {len(valid_prolific_pids)} unique Prolific IDs from Comparisons survey\")\n",
        "print(f\"Sample PIDs: {list(valid_prolific_pids)[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Filter Comparisons to Valid PIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pid_from_metadata(val):\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    match = re.search(r'prolific_pid=([a-f0-9]+)', str(val), re.IGNORECASE)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def get_study_id_from_metadata(val):\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    match = re.search(r'study_id=([^&]+)', str(val))\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "comparisons_df['prolific_pid'] = comparisons_df[meta_col].apply(get_pid_from_metadata)\n",
        "comparisons_filtered = comparisons_df[comparisons_df['prolific_pid'].isin(valid_prolific_pids)].copy()\n",
        "\n",
        "# If same PID appears multiple times (e.g. different study runs), keep the most recent\n",
        "comparisons_filtered = comparisons_filtered.sort_values('Timestamp').drop_duplicates(subset=['prolific_pid'], keep='last')\n",
        "\n",
        "print(f\"Comparisons after filtering: {len(comparisons_filtered)} rows (one per participant)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Comparisons: Histograms for 1–7 Scale Questions\n",
        "\n",
        "Scale: **1 = Graph preferred**, **7 = List preferred**\n",
        "\n",
        "Questions:\n",
        "- Which interface made the task easier overall?\n",
        "- Which interface felt more overwhelming to use?\n",
        "- Which interface better supported understanding the range and distribution of outputs?\n",
        "- Which interface made you feel more confident in your answers?\n",
        "- Overall, which interface do you prefer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COMPARISON_QUESTIONS = [\n",
        "    \"Which interface made the task easier overall?\",\n",
        "    \"Which interface felt more overwhelming to use?\",\n",
        "    \"Which interface better supported understanding the range and distribution of outputs?\",\n",
        "    \"Which interface made you feel more confident in your answers?\",\n",
        "    \"Overall, which interface do you prefer?\",\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, q in enumerate(COMPARISON_QUESTIONS):\n",
        "    ax = axes[i]\n",
        "    col = [c for c in comparisons_filtered.columns if q in c or c == q]\n",
        "    if not col:\n",
        "        ax.text(0.5, 0.5, f\"Column not found: {q[:40]}...\", ha='center', va='center')\n",
        "        continue\n",
        "    vals = pd.to_numeric(comparisons_filtered[col[0]], errors='coerce').dropna()\n",
        "    vals = vals[(vals >= 1) & (vals <= 7)]\n",
        "    ax.hist(vals, bins=np.arange(0.5, 8.5, 1), edgecolor='black', alpha=0.7)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax.set_xticks(range(1, 8))\n",
        "    ax.set_xticklabels(['1\\n(graph)', '2', '3', '4', '5', '6', '7\\n(list)'])\n",
        "    title = '\\n'.join(textwrap.wrap(q, width=50))\n",
        "    ax.set_title(title)\n",
        "\n",
        "axes[-1].axis('off')\n",
        "fig.suptitle('Comparisons: Graph vs List (1=Graph, 7=List)', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Single Distribution\n",
        "\n",
        "### 3.1 Filter to Valid PIDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filter rows to participants in our cohort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sd_meta_col = 'Metadata - no need to edit this'\n",
        "single_dist_df['prolific_pid'] = single_dist_df[sd_meta_col].apply(get_pid_from_metadata)\n",
        "single_dist_df['study_id'] = single_dist_df[sd_meta_col].apply(get_study_id_from_metadata)\n",
        "single_dist_filtered = single_dist_df[single_dist_df['prolific_pid'].isin(valid_prolific_pids)].copy()\n",
        "\n",
        "print(f\"Single Distribution: {len(single_dist_df)} total → {len(single_dist_filtered)} after PID filter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Split: Graph vs List\n",
        "\n",
        "Use \"Which interface were you using?\" and verify with `vis_type` from Metadata URL.\n",
        "- **Graph**: \"Graph\" in interface question AND `vis_type=graph` in metadata\n",
        "- **List**: \"List of outputs\" (or similar) AND `vis_type=raw_outputs` in metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse vis_type from metadata URL (format: ...&vis_type=raw_outputs or vis_type=graph)\n",
        "def parse_vis_from_metadata(val):\n",
        "    if pd.isna(val): return None\n",
        "    m = re.search(r'vis_type=([^,&]+)', str(val))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "single_dist_filtered['vis_type'] = single_dist_filtered[sd_meta_col].apply(parse_vis_from_metadata)\n",
        "\n",
        "# Map to graph vs list\n",
        "single_dist_filtered['interface'] = single_dist_filtered['vis_type'].map({\n",
        "    'graph': 'graph',\n",
        "    'raw_outputs': 'list'\n",
        "})\n",
        "\n",
        "# If vis_type parsing failed, use the question\n",
        "mask = single_dist_filtered['interface'].isna()\n",
        "single_dist_filtered.loc[mask, 'interface'] = single_dist_filtered.loc[mask, 'Which interface were you using?'].map(\n",
        "    lambda x: 'graph' if str(x).strip().lower() == 'graph' else ('list' if 'list' in str(x).lower() else None)\n",
        ")\n",
        "\n",
        "sd_graph = single_dist_filtered[single_dist_filtered['interface'] == 'graph'].copy()\n",
        "sd_list = single_dist_filtered[single_dist_filtered['interface'] == 'list'].copy()\n",
        "\n",
        "print(f\"Graph responses: {len(sd_graph)}\")\n",
        "print(f\"List responses: {len(sd_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.1 Condition counts: Interface × Task × Order\n",
        "\n",
        "Number of responses (and participants) in each combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sd_valid = single_dist_filtered[single_dist_filtered['interface'].notna()].copy()\n",
        "sd_valid = sd_valid.sort_values(['prolific_pid', 'study_id', 'Timestamp'])\n",
        "sd_valid['order'] = sd_valid.groupby(['prolific_pid', 'study_id']).cumcount() + 1  # 1 = Task 1, 2 = Task 2\n",
        "sd_valid['task_type'] = sd_valid[\"What were the questions about\"]\n",
        "sd_valid['interface_label'] = sd_valid['interface'].map({'graph': 'Graph', 'list': 'List'})\n",
        "\n",
        "print(\"Response-level counts: Interface × Task type × Order\")\n",
        "print(\"(Each row = one response; each participant has 2 responses: Task 1 and Task 2)\")\n",
        "counts = sd_valid.groupby(['interface_label', 'task_type', 'order']).size().unstack(fill_value=0)\n",
        "counts = counts.rename(columns={i: f'Task {i}' for i in counts.columns})\n",
        "print(counts.to_string())\n",
        "print()\n",
        "\n",
        "print(\"Participant-level conditions: Interface and dataset for Task 1\")\n",
        "print(\"(Determines full 2-task sequence per person)\")\n",
        "sd_first = sd_valid[sd_valid['order'] == 1]\n",
        "cond = sd_first['interface_label'].astype(str) + \" + \" + sd_first['task_type'].astype(str) + \" first\"\n",
        "for c, n in cond.value_counts().sort_index().items():\n",
        "    print(f\"  {c}: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Survey Question Columns\n",
        "\n",
        "We use columns by index to handle CSV encoding. **Exclude** \"This study is about elephant behavior\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column names for survey questions (by section). Skip elephant column.\n",
        "SECTIONS = {\n",
        "    \"Understanding model behavior for individual prompts\": [\n",
        "        \"\\\"Using this interface, I understood how diverse (ie, how narrow or broad) the output space was for a given prompt\\\"\",\n",
        "        \"\\\"I understood what a typical (or 'average') output looked like for a given prompt. \\\"\",\n",
        "        \"\\\"I had a good sense of what rare or unusual outputs looked like for a given prompt.\\\"\",\n",
        "        \"\\\"I felt I had seen enough of the model's behavior for a prompt to make a good decision.\\\"\",\n",
        "        \"\\\"I could see if there were recurring patterns in the outputs, and if so, what types\\\"\",\n",
        "    ],\n",
        "    \"Workload and Effort\": [\n",
        "        \"\\\"Using this interface required a lot of mental effort.\\\"\",\n",
        "        \"\\\"I had to work hard to complete the task using this interface.\\\"\",\n",
        "        \"\\\"I felt frustrated while using this interface.\\\"\",\n",
        "        \"\\\"I felt rushed while using this interface.\\\"\",\n",
        "    ],\n",
        "    \"Usability and satisfaction\": [\n",
        "        \"\\\"I found the interface easy to use.\\\"\",\n",
        "        \"\\\"I felt confident in the decisions I made using this interface.\\\"\",\n",
        "        \"\\\"I would want to use this interface for similar tasks in my own work.\\\"\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "def find_column(df, partial_name):\n",
        "    \"\"\"Find column that contains the partial name (handles encoding variations).\"\"\"\n",
        "    partial_clean = partial_name.replace('\\\\\"', '\"').strip()\n",
        "    for c in df.columns:\n",
        "        if partial_clean in c or c.strip() == partial_clean:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "# Build list of (section, question, column_name)\n",
        "survey_cols = []\n",
        "for section, questions in SECTIONS.items():\n",
        "    for q in questions:\n",
        "        col = find_column(single_dist_filtered, q.replace('\\\\\"', '\"'))\n",
        "        if col:\n",
        "            survey_cols.append((section, q[:60] + ('...' if len(q) > 60 else ''), col))\n",
        "        else:\n",
        "            # Try matching by key phrase\n",
        "            key = q.split(\",\")[0][:50] if ',' in q else q[:50]\n",
        "            for c in single_dist_filtered.columns:\n",
        "                if key.replace('\\\\\"', '').replace('\"', '') in c.replace('\"', ''):\n",
        "                    survey_cols.append((section, q[:60], c))\n",
        "                    break\n",
        "\n",
        "print(\"Resolved columns:\")\n",
        "for sec, q, col in survey_cols:\n",
        "    print(f\"  {sec}: {col[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Horizontal Stacked Bar Charts (Graph vs List)\n",
        "\n",
        "Same data as above, shown as horizontal stacked bars. Each question is a row; two paired bars per row (Graph, List), with response levels 1–7 stacked using a diverging palette."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Accuracy\n",
        "\n",
        "### 4.1 Ground Truth for Accuracy Questions\n",
        "\n",
        "The Single Distribution survey includes accuracy questions (not usability). We compute ground truth from the **first 20 outputs** of each dataset:\n",
        "- **user_study_monsters**: First prompt (child in magical world); outputs from `examples_user_study_monsters`\n",
        "- **user_study_places**: First prompt (explore magical world); outputs from `examples_user_study_places`\n",
        "\n",
        "We exclude \"write one sentence\" free-text questions. Ground truth answers are computed below for later accuracy comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Accuracy Analysis (by Interface)\n",
        "\n",
        "We compare user responses to ground truth for each accuracy question. **Filtering**: Only rows where the user answered about that dataset (Monsters vs Places from \"What were the questions about\").\n",
        "\n",
        "#### Methodology\n",
        "\n",
        "1. **Exact-match questions** (creature/location most frequent, theme, phrase, impossible, most likely sentence): 1 if correct, 0 if wrong. Theme and phrase use flexible string matching (strip quotes, case-insensitive) to handle minor variations.\n",
        "\n",
        "2. **Bucket questions** (percentage ranges: 0%, 1–5%, 6–15%, 16–30%, 31–50%, >50%): **Partial credit** based on bucket distance.\n",
        "   - Buckets ordered: `[0%, 1–5%, 6–15%, 16–30%, 31–50%, >50%]` (indices 0–5).\n",
        "   - Distance = `|truth_index - user_index|` (max 5).\n",
        "   - **Score = max(0, 1 - distance/5)**. So: exact match = 1.0; off by 1 bucket = 0.8; off by 2 = 0.6; off by 3 = 0.4; off by 4 = 0.2; off by 5 = 0.0.\n",
        "   - Rationale: Saying 6–15% when truth is 0% is less wrong than saying >50%; partial credit reflects that.\n",
        "\n",
        "3. **Normalization**: Bucket strings normalized (replace hyphens with en-dash, strip whitespace) before comparison.\n",
        "\n",
        "4. **Theme matching**: Flexible—exact match after normalizing, or ≥2 key words from the ground-truth theme present in the user answer (to handle paraphrasing).\n",
        "\n",
        "5. **Phrase matching**: Ground-truth phrase substring in user answer or vice versa (case-insensitive, quotes stripped). Note: Places ground truth is \"ancient druids\" (6/20 outputs); survey may offer \"ancient forest\" (4/20)—we use strict data-derived truth.\n",
        "\n",
        "6. **Impossible / Most likely**: Match by string prefix or substantial overlap (first 40–60 chars) to allow for minor truncation in logged responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column indices for accuracy questions (from Single Distribution CSV)\n",
        "MONSTERS_COLS = {\n",
        "    \"creature_most_freq\": 2,\n",
        "    \"creature_buckets\": [3, 4, 5, 6],  # Lumisprite, Luminara, Mistwraith, Lumivine\n",
        "    \"char_buckets\": [7, 8, 9, 10],     # Glowing, Guides, Moon/stars, Aggressive\n",
        "    \"theme\": 11,\n",
        "    \"phrase\": 12,\n",
        "    \"impossible\": 13,\n",
        "    \"most_likely\": 14,\n",
        "}\n",
        "PLACES_COLS = {\n",
        "    \"location_most_freq\": 16,\n",
        "    \"location_buckets\": [17, 18, 19, 20],  # Crystal Caverns, Whispering Glade, Shimmering Vale, Celestial Library\n",
        "    \"npc_buckets\": [21, 22, 23, 24],       # Merchants, Wizards, Druids, Elves\n",
        "    \"theme\": 25,\n",
        "    \"phrase\": 26,\n",
        "    \"impossible\": 27,\n",
        "    \"most_likely\": 28,\n",
        "}\n",
        "\n",
        "BUCKET_ORDER = [\"0%\", \"1–5%\", \"6–15%\", \"16–30%\", \"31–50%\", \">50%\"]\n",
        "\n",
        "def normalize_bucket(s):\n",
        "    \"\"\"Normalize bucket string for comparison (handle 1-5% vs 1–5%, etc.).\"\"\"\n",
        "    if pd.isna(s) or str(s).strip() == \"\": return None\n",
        "    s = str(s).strip()\n",
        "    s = s.replace(\"-\", \"–\")  # normalize hyphen to en-dash\n",
        "    if s in BUCKET_ORDER: return s\n",
        "    # Try to match\n",
        "    for b in BUCKET_ORDER:\n",
        "        if b.replace(\"–\", \"-\") in s or s.replace(\"-\", \"–\") == b:\n",
        "            return b\n",
        "    return None\n",
        "\n",
        "def bucket_index(bucket):\n",
        "    \"\"\"Get index of bucket in BUCKET_ORDER, or None if unknown.\"\"\"\n",
        "    b = normalize_bucket(bucket)\n",
        "    return BUCKET_ORDER.index(b) if b in BUCKET_ORDER else None\n",
        "\n",
        "def bucket_partial_credit(truth_bucket, user_bucket):\n",
        "    \"\"\"\n",
        "    Partial credit for bucket questions. Score = max(0, 1 - distance/5).\n",
        "    \"\"\"\n",
        "    ti, ui = bucket_index(truth_bucket), bucket_index(user_bucket)\n",
        "    if ti is None or ui is None: return np.nan\n",
        "    distance = abs(ti - ui)\n",
        "    return max(0.0, 1.0 - distance / 5.0)\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Strip quotes, whitespace, lowercase for flexible string matching.\"\"\"\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = str(s).strip().lower()\n",
        "    s = s.replace(\"\"\", '\"').replace(\"\"\", '\"').replace(\"'\", \"'\")\n",
        "    return s\n",
        "\n",
        "def theme_match(user_val, truth_val):\n",
        "    \"\"\"Flexible theme matching: exact after normalize, or key phrases present.\"\"\"\n",
        "    u, t = normalize_text(user_val), normalize_text(truth_val)\n",
        "    if u == t: return True\n",
        "    # Key phrases from truth (monsters: ethereal/glowing; places: ancient/druids)\n",
        "    key_words = t.split()[:5]  # first 5 words as proxy for main idea\n",
        "    return sum(1 for w in key_words if len(w) > 3 and w in u) >= 2\n",
        "\n",
        "def phrase_match(user_val, truth_val):\n",
        "    \"\"\"Phrase match: normalize and check if truth phrase is in user answer.\"\"\"\n",
        "    u, t = normalize_text(user_val), normalize_text(truth_val)\n",
        "    return t in u or u in t\n",
        "\n",
        "# Split by dataset and interface\n",
        "sd = single_dist_filtered.copy()\n",
        "cols = list(sd.columns)\n",
        "m_df = sd[sd[\"What were the questions about\"] == \"Monsters\"]\n",
        "p_df = sd[sd[\"What were the questions about\"] == \"Locations\"]\n",
        "\n",
        "def compute_accuracy(df, col_map, truth, dataset_name):\n",
        "    \"\"\"Compute accuracy scores for a dataset. Returns dict of question -> (scores_list, by_interface).\"\"\"\n",
        "    results = {}\n",
        "    if len(df) == 0: return results\n",
        "\n",
        "    # Creature/location most frequent\n",
        "    col = cols[col_map[\"creature_most_freq\" if \"monsters\" in dataset_name else \"location_most_freq\"]]\n",
        "    gt_val = truth[\"creature_most_frequent\" if \"monsters\" in dataset_name else \"location_most_frequent\"]\n",
        "    for iface in [\"graph\", \"list\"]:\n",
        "        subset = df[df[\"interface\"] == iface]\n",
        "        if len(subset) == 0: continue\n",
        "        vals = subset[col].apply(lambda x: normalize_text(x) == normalize_text(gt_val) if pd.notna(x) else False)\n",
        "        key = f\"most_frequent_entity_{iface}\"\n",
        "        results[key] = vals.astype(float).tolist()\n",
        "\n",
        "    # Bucket questions\n",
        "    bucket_keys = [\"creature_buckets\", \"char_buckets\"] if \"monsters\" in dataset_name else [\"location_buckets\", \"npc_buckets\"]\n",
        "    truth_buckets = {\"creature_buckets\": truth[\"creature_buckets\"], \"char_buckets\": truth[\"characteristic_buckets\"]} if \"monsters\" in dataset_name else {\"location_buckets\": truth[\"location_buckets\"], \"npc_buckets\": truth[\"npc_buckets\"]}\n",
        "    for bk, col_idxs in zip(bucket_keys, [col_map[\"creature_buckets\"], col_map[\"char_buckets\"]] if \"monsters\" in dataset_name else [col_map[\"location_buckets\"], col_map[\"npc_buckets\"]]):\n",
        "        gt_buckets = truth_buckets[bk]\n",
        "        for i, (col_idx, entity) in enumerate(zip(col_idxs, list(gt_buckets.keys()))):\n",
        "            col_name = cols[col_idx]\n",
        "            gt_b = gt_buckets[entity]\n",
        "            for iface in [\"graph\", \"list\"]:\n",
        "                subset = df[df[\"interface\"] == iface]\n",
        "                if len(subset) == 0: continue\n",
        "                scores = subset[col_name].apply(lambda u: bucket_partial_credit(gt_b, u))\n",
        "                key = f\"bucket_{entity}_{iface}\"\n",
        "                results[key] = scores.tolist()\n",
        "\n",
        "    # Theme\n",
        "    col = cols[col_map[\"theme\"]]\n",
        "    gt_theme = truth[\"theme\"]\n",
        "    for iface in [\"graph\", \"list\"]:\n",
        "        subset = df[df[\"interface\"] == iface]\n",
        "        if len(subset) == 0: continue\n",
        "        vals = subset[col].apply(lambda x: theme_match(x, gt_theme) if pd.notna(x) else False)\n",
        "        results[f\"theme_{iface}\"] = vals.astype(float).tolist()\n",
        "\n",
        "    # Phrase\n",
        "    col = cols[col_map[\"phrase\"]]\n",
        "    gt_phrase = truth[\"phrase_most_frequent\"]\n",
        "    for iface in [\"graph\", \"list\"]:\n",
        "        subset = df[df[\"interface\"] == iface]\n",
        "        if len(subset) == 0: continue\n",
        "        vals = subset[col].apply(lambda x: phrase_match(x, gt_phrase) if pd.notna(x) else False)\n",
        "        results[f\"phrase_{iface}\"] = vals.astype(float).tolist()\n",
        "\n",
        "    # Impossible (exact or startswith)\n",
        "    col = cols[col_map[\"impossible\"]]\n",
        "    gt_imp = truth[\"impossible\"]\n",
        "    for iface in [\"graph\", \"list\"]:\n",
        "        subset = df[df[\"interface\"] == iface]\n",
        "        if len(subset) == 0: continue\n",
        "        vals = subset[col].apply(lambda x: str(x).strip().startswith(gt_imp[:40]) if pd.notna(x) else False)\n",
        "        results[f\"impossible_{iface}\"] = vals.astype(float).tolist()\n",
        "\n",
        "    # Most likely sentence\n",
        "    col = cols[col_map[\"most_likely\"]]\n",
        "    gt_sent = truth[\"most_likely_sentence\"]\n",
        "    for iface in [\"graph\", \"list\"]:\n",
        "        subset = df[df[\"interface\"] == iface]\n",
        "        if len(subset) == 0: continue\n",
        "        vals = subset[col].apply(lambda x: normalize_text(str(x)[:60]) in normalize_text(gt_sent) or normalize_text(gt_sent)[:60] in normalize_text(str(x)) if pd.notna(x) else False)\n",
        "        results[f\"most_likely_{iface}\"] = vals.astype(float).tolist()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "# Allow import from user_study_results/ when running from repo root or notebook dir\n",
        "if str(DATA_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(DATA_DIR))\n",
        "from ground_truth_outputs import MONSTERS_FIRST_20, PLACES_FIRST_20\n",
        "\n",
        "def pct_to_bucket(pct):\n",
        "    \"\"\"Map percentage (0-100) to survey bucket.\"\"\"\n",
        "    if pct == 0: return \"0%\"\n",
        "    if pct <= 5: return \"1–5%\"\n",
        "    if pct <= 15: return \"6–15%\"\n",
        "    if pct <= 30: return \"16–30%\"\n",
        "    if pct <= 50: return \"31–50%\"\n",
        "    return \">50%\"\n",
        "\n",
        "def count_contains(outputs, needle, case_sensitive=False):\n",
        "    \"\"\"Count outputs that contain needle (substring).\"\"\"\n",
        "    n = needle if case_sensitive else needle.lower()\n",
        "    return sum(1 for o in outputs if (o if case_sensitive else o.lower()).find(n) >= 0)\n",
        "\n",
        "# --- MONSTERS GROUND TRUTH ---\n",
        "monsters = MONSTERS_FIRST_20\n",
        "n = len(monsters)\n",
        "\n",
        "# Creature names (survey options: The Lumisprite, The Luminara, The Mistwraith, The Lumivine)\n",
        "creature_names = [\"Lumisprite\", \"Luminara\", \"Mistwraith\", \"Lumivine\"]\n",
        "creature_counts = {c: count_contains(monsters, c) for c in creature_names}\n",
        "most_frequent_creature = \"The \" + max(creature_counts, key=creature_counts.get)  # Survey uses \"The Lumivine\" etc.\n",
        "\n",
        "monsters_creature_pcts = {c: (creature_counts[c] / n) * 100 for c in creature_names}\n",
        "monsters_creature_buckets = {c: pct_to_bucket(monsters_creature_pcts[c]) for c in creature_names}\n",
        "\n",
        "# Characteristics (Glowing/bioluminescent, Guides lost travelers, Born from moon or stars, Aggressive predators)\n",
        "char_phrases = [\n",
        "    (\"Glowing / bioluminescent creatures\", [\"glow\", \"glowing\", \"bioluminescent\", \"luminescent\", \"luminous\"]),\n",
        "    (\"Guides lost travelers\", [\"guide\", \"guiding\", \"guides\", \"traveler\", \"travelers\"]),\n",
        "    (\"Born from moon or stars\", [\"moon\", \"star\", \"stellar\", \"celestial\", \"fallen star\"]),\n",
        "    (\"Aggressive predators\", [\"aggressive\", \"predator\", \"devour\", \"attack\", \"hostile\", \"hunt\"]),\n",
        "]\n",
        "monsters_char_counts = {}\n",
        "for label, phrases in char_phrases:\n",
        "    cnt = sum(1 for o in monsters if any(p in o.lower() for p in phrases))\n",
        "    monsters_char_counts[label] = pct_to_bucket((cnt / n) * 100)\n",
        "\n",
        "# Most frequent phrase (survey options like \"guides lost travelers\", \"devours its prey\")\n",
        "phrase_options = [\"guides lost travelers\", \"devours its prey\", \"rules the battlefield\"]\n",
        "phrase_counts = {p: sum(1 for o in monsters if p in o.lower()) for p in phrase_options}\n",
        "most_frequent_phrase_monsters = max(phrase_counts, key=phrase_counts.get) if any(phrase_counts.values()) else \"guides lost travelers\"\n",
        "\n",
        "# Theme (Ethereal/glowing..., Fierce beasts..., Urban magical...)\n",
        "# From outputs: mostly ethereal/glowing forest guardians\n",
        "monsters_theme = \"Ethereal, glowing forest guardians that guide lost souls\"\n",
        "\n",
        "# Impossible: The Ironclad Ravager (steampunk) - NOT in distribution\n",
        "monsters_impossible = \"The Ironclad Ravager: A mechanical beast powered by steam engines that hunts travelers in abandoned cities.\"\n",
        "\n",
        "# Most likely sentence: The Lumivine is a bioluminescent serpent...\n",
        "monsters_most_likely = \"The Lumivine is a bioluminescent serpent that weaves through enchanted forests, guiding lost travelers beneath the glow of the moon.\"\n",
        "\n",
        "# --- PLACES GROUND TRUTH ---\n",
        "places = PLACES_FIRST_20\n",
        "n_place = len(places)\n",
        "\n",
        "# Location names (survey options: The Crystal Caverns, The Whispering Glade, The Shimmering Vale, The Celestial Library)\n",
        "location_names = [\"Crystal Caverns\", \"Whispering Glade\", \"Shimmering Vale\", \"Celestial Library\"]\n",
        "# Also match \"Whispering Glade\" without \"The\", \"Luminous Glade\" etc. - use canonical substring\n",
        "location_counts = {\n",
        "    \"Crystal Caverns\": count_contains(places, \"Crystal Cavern\"),\n",
        "    \"Whispering Glade\": count_contains(places, \"Whispering Glade\") + count_contains(places, \"Whispering Grove\"),\n",
        "    \"Shimmering Vale\": count_contains(places, \"Shimmering Vale\"),\n",
        "    \"Celestial Library\": count_contains(places, \"Celestial Library\"),\n",
        "}\n",
        "# Whispering Grove is a variant of Whispering Glade - already counted. Luminous Glade, Shimmering Glade are different.\n",
        "most_frequent_location = \"The \" + max(location_counts, key=location_counts.get)  # Survey uses \"The Whispering Glade\" etc.\n",
        "\n",
        "places_location_pcts = {c: (location_counts[c] / n_place) * 100 for c in location_names}\n",
        "places_location_buckets = {c: pct_to_bucket(places_location_pcts[c]) for c in location_names}\n",
        "\n",
        "# NPC types (Merchants, Wizards, Druids, Elves) - column header says \"creatures\" but values are these\n",
        "npc_phrases = [\"Merchants\", \"Wizards\", \"Druids\", \"Elves\"]\n",
        "npc_counts = {p: count_contains(places, p) for p in npc_phrases}\n",
        "places_npc_buckets = {p: pct_to_bucket((npc_counts[p] / n_place) * 100) for p in npc_phrases}\n",
        "\n",
        "# Phrase options for places (e.g. \"ancient forest\")\n",
        "phrase_options_places = [\"ancient forest\", \"ancient druids\", \"moon spirits\"]\n",
        "phrase_counts_places = {p: sum(1 for o in places if p in o.lower()) for p in phrase_options_places}\n",
        "most_frequent_phrase_places = max(phrase_counts_places, key=phrase_counts_places.get)\n",
        "\n",
        "# Theme for places\n",
        "places_theme = \"An ancient magical forest tied to druids or spirits\"\n",
        "\n",
        "# Impossible (futuristic): The Neon Citadel\n",
        "places_impossible = \"The Neon Citadel: A futuristic city powered by arcane machinery and ruled by technomancers wielding holographic spells.\"\n",
        "\n",
        "# Most likely sentence (places)\n",
        "places_most_likely = \"The Luminous Glade is a serene forest where ancient, glowing trees rise high, said to have been blessed by moon spirits to guide lost travelers and protect sacred knowledge hidden within.\"\n",
        "\n",
        "# --- CONSOLIDATED GROUND TRUTH (for accuracy comparison) ---\n",
        "GROUND_TRUTH = {\n",
        "    \"monsters\": {\n",
        "        \"creature_most_frequent\": most_frequent_creature,\n",
        "        \"creature_buckets\": monsters_creature_buckets,\n",
        "        \"characteristic_buckets\": monsters_char_counts,\n",
        "        \"theme\": monsters_theme,\n",
        "        \"phrase_most_frequent\": most_frequent_phrase_monsters,\n",
        "        \"impossible\": monsters_impossible,\n",
        "        \"most_likely_sentence\": monsters_most_likely,\n",
        "    },\n",
        "    \"places\": {\n",
        "        \"location_most_frequent\": most_frequent_location,\n",
        "        \"location_buckets\": places_location_buckets,\n",
        "        \"npc_buckets\": places_npc_buckets,\n",
        "        \"theme\": places_theme,\n",
        "        \"phrase_most_frequent\": most_frequent_phrase_places,\n",
        "        \"impossible\": places_impossible,\n",
        "        \"most_likely_sentence\": places_most_likely,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"=== MONSTERS GROUND TRUTH ===\\n\")\n",
        "print(\"1. Which creature name appeared most frequently?\", most_frequent_creature)\n",
        "print(\"2. Creature % buckets:\", monsters_creature_buckets)\n",
        "print(\"3. Characteristic % buckets:\", monsters_char_counts)\n",
        "print(\"4. Which theme?\", monsters_theme)\n",
        "print(\"5. Which phrase appeared most frequently?\", most_frequent_phrase_monsters)\n",
        "print(\"6. Impossible (distractor):\", monsters_impossible[:60] + \"...\")\n",
        "print(\"7. Most likely sentence:\", monsters_most_likely[:60] + \"...\")\n",
        "print(\"\\n=== PLACES GROUND TRUTH ===\\n\")\n",
        "print(\"1. Which location occurred most frequently?\", most_frequent_location)\n",
        "print(\"2. Location % buckets:\", places_location_buckets)\n",
        "print(\"3. NPC % buckets (Wizards, Druids, etc.):\", places_npc_buckets)\n",
        "print(\"4. Which theme?\", places_theme)\n",
        "print(\"5. Which phrase?\", most_frequent_phrase_places)\n",
        "print(\"6. Impossible:\", places_impossible[:60] + \"...\")\n",
        "print(\"7. Most likely sentence:\", places_most_likely[:60] + \"...\")\n",
        "print(\"\\n--- GROUND_TRUTH dict ready for accuracy comparison ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run accuracy computation for both datasets\n",
        "monsters_results = compute_accuracy(m_df, MONSTERS_COLS, GROUND_TRUTH[\"monsters\"], \"monsters\")\n",
        "places_results = compute_accuracy(p_df, PLACES_COLS, GROUND_TRUTH[\"places\"], \"places\")\n",
        "\n",
        "# Build summary DataFrame for all questions\n",
        "def mean_n(vals):\n",
        "    clean = [v for v in vals if not (isinstance(v, float) and np.isnan(v))]\n",
        "    return (np.mean(clean), len(clean)) if clean else (np.nan, 0)\n",
        "\n",
        "rows = []\n",
        "for dataset, res in [(\"Monsters\", monsters_results), (\"Places\", places_results)]:\n",
        "    all_labels = sorted(set(k.replace(\"_graph\", \"\").replace(\"_list\", \"\") for k in res))\n",
        "    for label in all_labels:\n",
        "        g_m, g_n = mean_n(res.get(label + \"_graph\", []))\n",
        "        l_m, l_n = mean_n(res.get(label + \"_list\", []))\n",
        "        rows.append({\n",
        "            \"Dataset\": dataset,\n",
        "            \"Question\": label.replace(\"_\", \" \").replace(\"bucket \", \"bucket: \"),\n",
        "            \"Graph_mean\": g_m,\n",
        "            \"Graph_n\": g_n,\n",
        "            \"List_mean\": l_m,\n",
        "            \"List_n\": l_n,\n",
        "        })\n",
        "\n",
        "accuracy_df = pd.DataFrame(rows)\n",
        "\n",
        "# Display\n",
        "print(\"=\" * 90)\n",
        "print(\"ACCURACY BY INTERFACE (Graph vs List)\")\n",
        "print(\"=\" * 90)\n",
        "print(\"\\nScoring: Binary (0/1) for exact-match questions; partial credit for bucket questions.\")\n",
        "print(\"Partial credit: score = max(0, 1 - |truth_idx - user_idx|/5) for buckets [0%, 1-5%, ..., >50%].\")\n",
        "print()\n",
        "\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    subset = accuracy_df[accuracy_df[\"Dataset\"] == dataset]\n",
        "    print(f\"\\n--- {dataset} ---\")\n",
        "    for _, r in subset.iterrows():\n",
        "        g_str = f\"{r['Graph_mean']:.3f} (n={int(r['Graph_n'])})\" if not np.isnan(r['Graph_mean']) else \"-\"\n",
        "        l_str = f\"{r['List_mean']:.3f} (n={int(r['List_n'])})\" if not np.isnan(r['List_mean']) else \"-\"\n",
        "        print(f\"  {r['Question']:45} Graph: {g_str:18}   List: {l_str}\")\n",
        "\n",
        "# Aggregated: mean accuracy across question types (exact-match vs bucket)\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"AGGREGATE: Mean accuracy by question type (averaged across sub-questions)\")\n",
        "print(\"=\" * 90)\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    sub = accuracy_df[accuracy_df[\"Dataset\"] == dataset]\n",
        "    # Group bucket questions\n",
        "    bucket_rows = sub[sub[\"Question\"].str.startswith(\"bucket:\")]\n",
        "    exact_rows = sub[~sub[\"Question\"].str.startswith(\"bucket:\")]\n",
        "    def wmean(df, which):  # which = \"Graph\" or \"List\"\n",
        "        col_mean, col_n = f\"{which}_mean\", f\"{which}_n\"\n",
        "        total = 0.0\n",
        "        count = 0\n",
        "        for _, r in df.iterrows():\n",
        "            n = r[col_n]\n",
        "            m = r[col_mean]\n",
        "            if n > 0 and not np.isnan(m):\n",
        "                total += m * n\n",
        "                count += n\n",
        "        return total / count if count > 0 else np.nan\n",
        "    print(f\"\\n{dataset}:\")\n",
        "    print(f\"  Bucket questions (partial credit): Graph {wmean(bucket_rows,'Graph'):.3f}, List {wmean(bucket_rows,'List'):.3f}\")\n",
        "    print(f\"  Exact-match questions:             Graph {wmean(exact_rows,'Graph'):.3f}, List {wmean(exact_rows,'List'):.3f}\")\n",
        "\n",
        "accuracy_df  # Return for further analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Accuracy Plots (Graph vs List)\n",
        "\n",
        "Paired bar charts for accuracy by question. Bucket sub-questions (e.g., Lumisprite, Luminara, ...) are combined into single \"Creature buckets\" / \"Characteristic buckets\" etc. using weighted means. Also includes overall average accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map bucket sub-questions to combined labels (for grouping)\n",
        "BUCKET_GROUPS = {\n",
        "    \"Monsters\": {\n",
        "        \"Creature buckets\": [\"Lumisprite\", \"Luminara\", \"Mistwraith\", \"Lumivine\"],\n",
        "        \"Characteristic buckets\": [\n",
        "            \"Glowing / bioluminescent creatures\",\n",
        "            \"Guides lost travelers\",\n",
        "            \"Born from moon or stars\",\n",
        "            \"Aggressive predators\",\n",
        "        ],\n",
        "    },\n",
        "    \"Places\": {\n",
        "        \"Location buckets\": [\"Crystal Caverns\", \"Whispering Glade\", \"Shimmering Vale\", \"Celestial Library\"],\n",
        "        \"NPC buckets\": [\"Merchants\", \"Wizards\", \"Druids\", \"Elves\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "def wmean_rows(df, rows, which):\n",
        "    \"\"\"Weighted mean across rows. which = 'Graph' or 'List'.\"\"\"\n",
        "    col_mean, col_n = f\"{which}_mean\", f\"{which}_n\"\n",
        "    total = 0.0\n",
        "    count = 0\n",
        "    for _, r in rows.iterrows():\n",
        "        n = r[col_n]\n",
        "        m = r[col_mean]\n",
        "        if n > 0 and not np.isnan(m):\n",
        "            total += m * n\n",
        "            count += n\n",
        "    return total / count if count > 0 else np.nan, count\n",
        "\n",
        "def se_from_scores(scores):\n",
        "    \"\"\"Standard error of the mean. Handles NaN.\"\"\"\n",
        "    clean = [s for s in scores if not (isinstance(s, float) and np.isnan(s))]\n",
        "    if len(clean) < 2:\n",
        "        return 0.0\n",
        "    return np.std(clean, ddof=1) / np.sqrt(len(clean))\n",
        "\n",
        "def question_to_label(q):\n",
        "    \"\"\"Map displayed Question back to results key base.\"\"\"\n",
        "    if q.startswith(\"bucket:\"):\n",
        "        entity = q.replace(\"bucket:\", \"\").strip()\n",
        "        return \"bucket_\" + entity\n",
        "    return q.replace(\" \", \"_\")\n",
        "\n",
        "def pvalue_graph_vs_list(g_scores, l_scores):\n",
        "    \"\"\"Independent t-test. Returns p-value or 1.0 if insufficient data.\"\"\"\n",
        "    g = [s for s in g_scores if not (isinstance(s, float) and np.isnan(s))]\n",
        "    l = [s for s in l_scores if not (isinstance(s, float) and np.isnan(s))]\n",
        "    if len(g) < 2 or len(l) < 2:\n",
        "        return 1.0\n",
        "    _, p = ttest_ind(g, l)\n",
        "    return float(p)\n",
        "\n",
        "# Build plot_df: combined bucket groups + exact-match questions + overall (with SE)\n",
        "results_map = {\"Monsters\": monsters_results, \"Places\": places_results}\n",
        "plot_rows = []\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    res = results_map[dataset]\n",
        "    sub = accuracy_df[accuracy_df[\"Dataset\"] == dataset].copy()\n",
        "    bucket_sub = sub[sub[\"Question\"].str.startswith(\"bucket:\")]\n",
        "    exact_sub = sub[~sub[\"Question\"].str.startswith(\"bucket:\")]\n",
        "\n",
        "    # Combined bucket groups\n",
        "    for group_name, entity_names in BUCKET_GROUPS[dataset].items():\n",
        "        matches = bucket_sub[\n",
        "            bucket_sub[\"Question\"].str.extract(r\"bucket:\\s*(.+)\", expand=False).str.strip().isin(entity_names)\n",
        "        ]\n",
        "        if len(matches) > 0:\n",
        "            g_m, g_n = wmean_rows(matches, matches, \"Graph\")\n",
        "            l_m, l_n = wmean_rows(matches, matches, \"List\")\n",
        "            # SE from per-participant average across sub-questions\n",
        "            g_keys = [f\"bucket_{e}_graph\" for e in entity_names if f\"bucket_{e}_graph\" in res]\n",
        "            l_keys = [f\"bucket_{e}_list\" for e in entity_names if f\"bucket_{e}_list\" in res]\n",
        "            g_scores = np.mean([res[k] for k in g_keys], axis=0) if g_keys else []\n",
        "            l_scores = np.mean([res[k] for k in l_keys], axis=0) if l_keys else []\n",
        "            g_se = se_from_scores(g_scores)\n",
        "            l_se = se_from_scores(l_scores)\n",
        "            plot_rows.append({\n",
        "                \"Dataset\": dataset,\n",
        "                \"Question\": group_name,\n",
        "                \"Graph_mean\": g_m,\n",
        "                \"Graph_se\": g_se,\n",
        "                \"Graph_n\": int(g_n),\n",
        "                \"List_mean\": l_m,\n",
        "                \"List_se\": l_se,\n",
        "                \"List_n\": int(l_n),\n",
        "                \"p_value\": pvalue_graph_vs_list(g_scores, l_scores),\n",
        "            })\n",
        "\n",
        "    # Exact-match questions as-is\n",
        "    for _, r in exact_sub.iterrows():\n",
        "        label = question_to_label(r[\"Question\"])\n",
        "        g_scores = res.get(label + \"_graph\", [])\n",
        "        l_scores = res.get(label + \"_list\", [])\n",
        "        g_se = se_from_scores(g_scores)\n",
        "        l_se = se_from_scores(l_scores)\n",
        "        plot_rows.append({\n",
        "            \"Dataset\": r[\"Dataset\"],\n",
        "            \"Question\": r[\"Question\"],\n",
        "            \"Graph_mean\": r[\"Graph_mean\"],\n",
        "            \"Graph_se\": g_se,\n",
        "            \"Graph_n\": int(r[\"Graph_n\"]),\n",
        "            \"List_mean\": r[\"List_mean\"],\n",
        "            \"List_se\": l_se,\n",
        "            \"List_n\": int(r[\"List_n\"]),\n",
        "            \"p_value\": pvalue_graph_vs_list(g_scores, l_scores),\n",
        "        })\n",
        "\n",
        "plot_df = pd.DataFrame(plot_rows)\n",
        "\n",
        "# Add overall row per dataset (SE from per-participant mean across all questions)\n",
        "overall_rows = []\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    res = results_map[dataset]\n",
        "    sub = plot_df[plot_df[\"Dataset\"] == dataset]\n",
        "    g_m, g_n = wmean_rows(sub, sub, \"Graph\")\n",
        "    l_m, l_n = wmean_rows(sub, sub, \"List\")\n",
        "    g_keys = [k for k in res if k.endswith(\"_graph\")]\n",
        "    l_keys = [k for k in res if k.endswith(\"_list\")]\n",
        "    g_scores = np.mean([res[k] for k in g_keys], axis=0) if g_keys else []\n",
        "    l_scores = np.mean([res[k] for k in l_keys], axis=0) if l_keys else []\n",
        "    g_se = se_from_scores(g_scores)\n",
        "    l_se = se_from_scores(l_scores)\n",
        "    overall_rows.append({\n",
        "        \"Dataset\": dataset,\n",
        "        \"Question\": \"Overall\",\n",
        "        \"Graph_mean\": g_m,\n",
        "        \"Graph_se\": g_se,\n",
        "        \"Graph_n\": int(g_n),\n",
        "        \"List_mean\": l_m,\n",
        "        \"List_se\": l_se,\n",
        "        \"List_n\": int(l_n),\n",
        "        \"p_value\": pvalue_graph_vs_list(g_scores, l_scores),\n",
        "    })\n",
        "overall_df = pd.DataFrame(overall_rows)\n",
        "\n",
        "# Plot 1: Paired bar chart by question (excluding overall)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, max(6, len(plot_df) * 0.35)))\n",
        "palette = sns.color_palette(\"muted\")\n",
        "graph_color, list_color = palette[0], palette[1]\n",
        "\n",
        "for ax_idx, dataset in enumerate([\"Monsters\", \"Places\"]):\n",
        "    ax = axes[ax_idx]\n",
        "    sub = plot_df[plot_df[\"Dataset\"] == dataset]\n",
        "    n = len(sub)\n",
        "    y = np.arange(n)\n",
        "    width = 0.35\n",
        "    g_se = sub[\"Graph_se\"].fillna(0).values\n",
        "    l_se = sub[\"List_se\"].fillna(0).values\n",
        "    ax.barh(y - width/2, sub[\"Graph_mean\"], width, xerr=g_se, label=\"Graph\", color=graph_color, edgecolor=\"black\", linewidth=0.5, capsize=2)\n",
        "    ax.barh(y + width/2, sub[\"List_mean\"], width, xerr=l_se, label=\"List\", color=list_color, edgecolor=\"black\", linewidth=0.5, capsize=2)\n",
        "    labels = [f\"{q} (y)\" if row.get(\"p_value\", 1) < 0.05 else f\"{q} (n)\" for q, (_, row) in zip(sub[\"Question\"], sub.iterrows())]\n",
        "    ax.set_yticks(y)\n",
        "    ax.set_yticklabels(labels, fontsize=9)\n",
        "    ax.set_xlim(0, 1.05)\n",
        "    ax.set_xlabel(\"Accuracy\")\n",
        "    ax.set_title(f\"{dataset} – Accuracy by Question (Graph vs List)\")\n",
        "    ax.legend(loc=\"lower right\", fontsize=8)\n",
        "    ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Paired bar chart for overall average only\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "x = np.arange(2)  # Monsters, Places\n",
        "width = 0.35\n",
        "g_se = overall_df[\"Graph_se\"].fillna(0).values\n",
        "l_se = overall_df[\"List_se\"].fillna(0).values\n",
        "ax.bar(x - width/2, overall_df[\"Graph_mean\"], width, yerr=g_se, label=\"Graph\", color=graph_color, edgecolor=\"black\", capsize=4)\n",
        "ax.bar(x + width/2, overall_df[\"List_mean\"], width, yerr=l_se, label=\"List\", color=list_color, edgecolor=\"black\", capsize=4)\n",
        "overall_labels = [f\"{d} (y)\" if row.get(\"p_value\", 1) < 0.05 else f\"{d} (n)\" for d, (_, row) in zip(overall_df[\"Dataset\"], overall_df.iterrows())]\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(overall_labels)\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_xlabel(\"Dataset\")\n",
        "ax.set_title(\"Overall Average Accuracy (Graph vs List)\")\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.axhline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
        "# Add value labels on bars\n",
        "for i, row in overall_df.iterrows():\n",
        "    ax.text(i - width/2, row[\"Graph_mean\"] + 0.02, f'{row[\"Graph_mean\"]:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "    ax.text(i + width/2, row[\"List_mean\"] + 0.02, f'{row[\"List_mean\"]:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use column indices if name matching is tricky (pandas standardizes)\n",
        "# Survey cols: 32,33,34,35, 37,38,39,40,41, 42,43,44 (skip 36=elephant)\n",
        "SD_SURVEY_COL_INDICES = [32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44]\n",
        "SD_SURVEY_SECTIONS = (\n",
        "    [\"Understanding model behavior\"] * 5 +\n",
        "    [\"Workload and Effort\"] * 4 +\n",
        "    [\"Usability and satisfaction\"] * 3\n",
        ")\n",
        "\n",
        "def plot_section_histograms_paired(sd_graph, sd_list):\n",
        "    \"\"\"Plot paired histograms: Graph vs List side-by-side for each question.\n",
        "    Single chart per section: each question gets a row with two histograms (Graph | List)\n",
        "    sharing a centered question header above each pair.\"\"\"\n",
        "    palette = sns.color_palette(\"muted\")\n",
        "    graph_color, list_color = palette[0], palette[1]\n",
        "    cols = [sd_graph.columns[i] for i in SD_SURVEY_COL_INDICES]\n",
        "    x_labels = ['1\\n(Strongly disagree)', '2', '3', '4', '5', '6', '7\\n(Strongly agree)']\n",
        "    bins = np.arange(0.5, 8.5, 1)\n",
        "\n",
        "    for section in [\"Understanding model behavior\", \"Workload and Effort\", \"Usability and satisfaction\"]:\n",
        "        indices = [i for i, s in enumerate(SD_SURVEY_SECTIONS) if s == section]\n",
        "        section_cols = [cols[i] for i in indices]\n",
        "        n = len(section_cols)\n",
        "        # One row per question, 2 columns: Graph | List (side by side)\n",
        "        fig, axes = plt.subplots(n, 2, figsize=(10, 4 * n))\n",
        "        if n == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "        fig.suptitle(section, y=1.01)\n",
        "        for i, col in enumerate(section_cols):\n",
        "            ax_g, ax_l = axes[i, 0], axes[i, 1]\n",
        "            vals_graph = pd.to_numeric(sd_graph[col], errors='coerce').dropna()\n",
        "            vals_graph = vals_graph[(vals_graph >= 1) & (vals_graph <= 7)]\n",
        "            vals_list = pd.to_numeric(sd_list[col], errors='coerce').dropna()\n",
        "            vals_list = vals_list[(vals_list >= 1) & (vals_list <= 7)]\n",
        "            counts_graph, _ = np.histogram(vals_graph, bins=bins)\n",
        "            counts_list, _ = np.histogram(vals_list, bins=bins)\n",
        "            avg_graph = vals_graph.mean() if len(vals_graph) > 0 else np.nan\n",
        "            avg_list = vals_list.mean() if len(vals_list) > 0 else np.nan\n",
        "            avg_g_str = f\"{avg_graph:.1f}\" if not np.isnan(avg_graph) else \"—\"\n",
        "            avg_l_str = f\"{avg_list:.1f}\" if not np.isnan(avg_list) else \"—\"\n",
        "            pct_agree_g = (vals_graph >= 5).sum() / len(vals_graph) * 100 if len(vals_graph) > 0 else np.nan\n",
        "            pct_agree_l = (vals_list >= 5).sum() / len(vals_list) * 100 if len(vals_list) > 0 else np.nan\n",
        "            pct_g_str = f\"{pct_agree_g:.0f}\" if not np.isnan(pct_agree_g) else \"—\"\n",
        "            pct_l_str = f\"{pct_agree_l:.0f}\" if not np.isnan(pct_agree_l) else \"—\"\n",
        "            x = np.arange(1, 8)\n",
        "            title = '\\n'.join(textwrap.wrap(col, width=55))\n",
        "            ax_g.bar(x, counts_graph, color=graph_color)\n",
        "            ax_g.set_title(f'$\\\\mathbf{{Graph}}$\\navg: $\\\\mathbf{{{avg_g_str}}}$\\ntotal agree: $\\\\mathbf{{{pct_g_str}}}\\\\%$', fontsize=9)\n",
        "            ax_g.set_ylabel('Count')\n",
        "            ax_g.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "            ax_g.set_xticks(range(1, 8))\n",
        "            ax_g.set_xticklabels(x_labels)\n",
        "            ax_g.tick_params(axis='x', labelsize=8)\n",
        "            ax_g.set_ylim(0, 15)\n",
        "            ax_l.bar(x, counts_list, color=list_color)\n",
        "            ax_l.set_title(f'$\\\\mathbf{{List}}$\\navg: $\\\\mathbf{{{avg_l_str}}}$\\ntotal agree: $\\\\mathbf{{{pct_l_str}}}\\\\%$', fontsize=9)\n",
        "            ax_l.set_ylabel('Count')\n",
        "            ax_l.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "            ax_l.set_xticks(range(1, 8))\n",
        "            ax_l.set_xticklabels(x_labels)\n",
        "            ax_l.tick_params(axis='x', labelsize=8)\n",
        "            ax_l.set_ylim(0, 15)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "        plt.subplots_adjust(hspace=0.45, wspace=0.25)\n",
        "        # Add shared question headers centered above each pair\n",
        "        for i, col in enumerate(section_cols):\n",
        "            title = '\\n'.join(textwrap.wrap(col, width=55))\n",
        "            bbox = axes[i, 0].get_position()\n",
        "            y_pos = bbox.ymax + 0.02\n",
        "            if y_pos > 1:\n",
        "                y_pos = 0.99\n",
        "            fig.text(0.5, y_pos, title, ha='center', wrap=True)\n",
        "        plt.show()\n",
        "\n",
        "plot_section_histograms_paired(sd_graph, sd_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stacked bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_section_stacked_bars(sd_graph, sd_list):\n",
        "    \"\"\"Horizontal stacked bar charts: for each question, two stacked bars (Graph on top, List below).\n",
        "    Question text above each pair, Graph/List labels on all bars. Blue=7, red=1. % labels in white.\"\"\"\n",
        "    diverging = list(sns.color_palette(\"RdBu_r\", 7))[::-1]  # 1=red, 7=blue (reversed)\n",
        "    cols = [sd_graph.columns[i] for i in SD_SURVEY_COL_INDICES]\n",
        "    row_stride = 2.1\n",
        "    bar_height = 0.48\n",
        "    bar_gap = 0.08\n",
        "    question_pad = 0.1\n",
        "    from matplotlib.transforms import blended_transform_factory\n",
        "    for section in [\"Understanding model behavior\", \"Workload and Effort\", \"Usability and satisfaction\"]:\n",
        "        indices = [i for i, s in enumerate(SD_SURVEY_SECTIONS) if s == section]\n",
        "        section_cols = [cols[i] for i in indices]\n",
        "        n = len(section_cols)\n",
        "        fig, ax = plt.subplots(figsize=(10, max(5, n * 1.15)))\n",
        "        t = blended_transform_factory(ax.transAxes, ax.transData)\n",
        "        for i, col in enumerate(section_cols):\n",
        "            y_graph = i * row_stride\n",
        "            y_list = i * row_stride + bar_height + bar_gap\n",
        "            vals_graph = pd.to_numeric(sd_graph[col], errors='coerce').dropna()\n",
        "            vals_graph = vals_graph[(vals_graph >= 1) & (vals_graph <= 7)].astype(int)\n",
        "            vals_list = pd.to_numeric(sd_list[col], errors='coerce').dropna()\n",
        "            vals_list = vals_list[(vals_list >= 1) & (vals_list <= 7)].astype(int)\n",
        "            counts_graph = vals_graph.value_counts().reindex(range(1, 8), fill_value=0).values\n",
        "            counts_list = vals_list.value_counts().reindex(range(1, 8), fill_value=0).values\n",
        "            props_graph = counts_graph / counts_graph.sum() if counts_graph.sum() > 0 else np.zeros(7)\n",
        "            props_list = counts_list / counts_list.sum() if counts_list.sum() > 0 else np.zeros(7)\n",
        "            left_g, left_l = 0.0, 0.0\n",
        "            for k in range(7):\n",
        "                ax.barh(y_graph, props_graph[k], height=bar_height, left=left_g,\n",
        "                        color=diverging[k], edgecolor='white', linewidth=0.4)\n",
        "                ax.barh(y_list, props_list[k], height=bar_height, left=left_l,\n",
        "                        color=diverging[k], edgecolor='white', linewidth=0.4)\n",
        "                if props_graph[k] >= 0.03:\n",
        "                    ax.text(left_g + props_graph[k] / 2, y_graph, f'{props_graph[k] * 100:.0f}%',\n",
        "                            ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
        "                if props_list[k] >= 0.03:\n",
        "                    ax.text(left_l + props_list[k] / 2, y_list, f'{props_list[k] * 100:.0f}%',\n",
        "                            ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
        "                left_g += props_graph[k]\n",
        "                left_l += props_list[k]\n",
        "            ax.text(-0.025, y_graph, 'Graph', transform=t, fontsize=7, va='center', ha='right', color='#555')\n",
        "            ax.text(-0.025, y_list, 'List', transform=t, fontsize=7, va='center', ha='right', color='#555')\n",
        "            ax.text(0.5, y_list + bar_height / 2 + question_pad, col, transform=t, fontsize=8, va='bottom', ha='center')\n",
        "        ax.set_yticks([])\n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        y_lo = -bar_height - 0.3\n",
        "        y_hi = (n - 1) * row_stride + bar_height + bar_gap + bar_height + 0.5\n",
        "        ax.set_ylim(y_lo, y_hi)\n",
        "        legend_labels = ['1\\n(strongly disagree)', '2', '3', '4', '5', '6', '7\\n(strongly agree)']\n",
        "        legend_handles = [plt.Rectangle((0, 0), 1, 1, fc=diverging[k], label=legend_labels[k]) for k in range(7)]\n",
        "        ax.legend(handles=legend_handles, title='Response', loc='upper center', bbox_to_anchor=(0.5, -0.18),\n",
        "                  ncol=7, frameon=False, fontsize=8)\n",
        "        ax.set_title(section, fontsize=12, pad=12)\n",
        "        plt.tight_layout(rect=[0.02, 0.18, 0.98, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "plot_section_stacked_bars(sd_graph, sd_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Timing by Interface and Dataset\n",
        "\n",
        "From outputs.csv telemetry: time between consecutive \"next\" navigations on survey pages. Filter by (prolific_pid, study_id) from cohort. Each segment attributed to (toDataset, toVisType)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Extract PID/Study Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_pid_study_pairs(metadata_series):\n",
        "    \"\"\"Extract (prolific_pid, study_id) pairs from metadata URL strings.\"\"\"\n",
        "    pairs = set()\n",
        "    for val in metadata_series.dropna():\n",
        "        s = str(val)\n",
        "        pid_m = re.search(r'prolific_pid=([a-f0-9]+)', s, re.IGNORECASE)\n",
        "        sid_m = re.search(r'study_id=([^&]+)', s)\n",
        "        if pid_m and sid_m:\n",
        "            pairs.add((pid_m.group(1), sid_m.group(1)))\n",
        "    return pairs\n",
        "\n",
        "sd_meta = [c for c in single_dist_df.columns if 'Metadata' in c][0]\n",
        "comp_meta = [c for c in comparisons_df.columns if 'Metadata' in c][0]\n",
        "all_pairs = extract_pid_study_pairs(single_dist_df[sd_meta]) | extract_pid_study_pairs(comparisons_df[comp_meta])\n",
        "valid_pid_study_pairs = {(p, s) for (p, s) in all_pairs if p in valid_prolific_pids}\n",
        "print(f\"Valid (pid, study_id) pairs: {len(valid_pid_study_pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Parse Telemetry & Build Timing DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_participant_id(pid_str):\n",
        "    \"\"\"Parse 'pid_session_study' -> (pid, study_id) or None.\"\"\"\n",
        "    if pd.isna(pid_str) or not pid_str or '_' not in str(pid_str):\n",
        "        return None\n",
        "    parts = str(pid_str).strip().split('_')\n",
        "    if len(parts) >= 3:\n",
        "        return (parts[0], parts[2])\n",
        "    return None\n",
        "\n",
        "def extract_timing_from_telemetry(telemetry_str):\n",
        "    \"\"\"Parse telemetry JSON, get time between consecutive 'next' events on task pages.\n",
        "    Returns list of (dataset, vis_type, time_sec).\"\"\"\n",
        "    try:\n",
        "        events = json.loads(telemetry_str)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return []\n",
        "    navs = [e for e in events if e.get('type') == 'landing_page_nav' and e.get('data', {}).get('action') == 'next']\n",
        "    navs = sorted(navs, key=lambda x: x['timestamp'])\n",
        "    result = []\n",
        "    for i in range(len(navs) - 1):\n",
        "        d = navs[i].get('data', {})\n",
        "        to_ds, to_vt = d.get('toDataset'), d.get('toVisType')\n",
        "        if to_ds and to_vt and 'user_study' in str(to_ds):\n",
        "            time_ms = navs[i + 1]['timestamp'] - navs[i]['timestamp']\n",
        "            if time_ms > 0 and time_ms < 3600 * 1000:  # sanity: under 1 hour\n",
        "                result.append((to_ds, to_vt, time_ms / 1000))\n",
        "    return result\n",
        "\n",
        "# Filter outputs to cohort, get best row per (pid, study_id)\n",
        "out_with_pid = outputs_df[outputs_df['Participant ID'].notna()].copy()\n",
        "out_with_pid = out_with_pid.assign(parsed=out_with_pid['Participant ID'].apply(parse_participant_id))\n",
        "out_with_pid = out_with_pid[out_with_pid['parsed'].notna()]\n",
        "out_with_pid['pid'] = out_with_pid['parsed'].apply(lambda x: x[0])\n",
        "out_with_pid['study_id'] = out_with_pid['parsed'].apply(lambda x: x[1])\n",
        "out_filtered = out_with_pid[out_with_pid.apply(lambda r: (r['pid'], r['study_id']) in valid_pid_study_pairs, axis=1)].copy()\n",
        "\n",
        "# Best telemetry per (pid, study_id): row with most events\n",
        "def event_count(telemetry_str):\n",
        "    try:\n",
        "        return len(json.loads(telemetry_str))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "out_filtered['n_events'] = out_filtered['Telemetry'].apply(event_count)\n",
        "best_rows = out_filtered.loc[out_filtered.groupby(['pid', 'study_id'])['n_events'].idxmax()]\n",
        "\n",
        "# Extract all timing segments (task_idx: 0=first task, 1=second task) with participant ids\n",
        "timing_rows = []\n",
        "for _, row in best_rows.iterrows():\n",
        "    for task_idx, (ds, vt, t) in enumerate(extract_timing_from_telemetry(row['Telemetry'])):\n",
        "        dataset_label = 'monsters' if 'monsters' in ds else 'places'\n",
        "        interface = 'Graph' if vt == 'graph' else 'List'\n",
        "        timing_rows.append({'time_sec': t, 'dataset': dataset_label, 'interface': interface, 'task_idx': task_idx, 'pid': row['pid'], 'study_id': row['study_id']})\n",
        "\n",
        "timing_df = pd.DataFrame(timing_rows)\n",
        "timing_task2_df = timing_df[timing_df['task_idx'] == 1].copy() if 'task_idx' in timing_df.columns else pd.DataFrame()\n",
        "print(f\"Timing segments: {len(timing_df)} (task 2: {len(timing_task2_df)})\")\n",
        "if len(timing_df) > 0:\n",
        "    print(timing_df.groupby(['interface', 'dataset']).agg(count=('time_sec', 'count'), mean_sec=('time_sec', 'mean')).round(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Plot Timing Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_timing_summary(timing_df):\n",
        "    \"\"\"Paired bar charts with mean and SEM error bars. Time in minutes.\"\"\"\n",
        "    if timing_df.empty:\n",
        "        print(\"No timing data to plot.\")\n",
        "        return\n",
        "    palette = sns.color_palette(\"muted\")\n",
        "    graph_color, list_color = palette[0], palette[1]\n",
        "    df = timing_df.copy()\n",
        "    df['time_min'] = df['time_sec'] / 60\n",
        "\n",
        "    # Plot 1: Paired bars by interface × dataset\n",
        "    agg = df.groupby(['interface', 'dataset'])['time_min'].agg(['mean', 'sem', 'count'])\n",
        "    datasets = sorted(df['dataset'].unique())\n",
        "    if len(datasets) == 0:\n",
        "        return\n",
        "    x = np.arange(len(datasets))\n",
        "    width = 0.35\n",
        "    timing_sig = []\n",
        "    fig, ax = plt.subplots(figsize=(9, 5))\n",
        "    for i, d in enumerate(datasets):\n",
        "        g_row = agg.loc[('Graph', d)] if ('Graph', d) in agg.index else pd.Series({'mean': np.nan, 'sem': 0})\n",
        "        l_row = agg.loc[('List', d)] if ('List', d) in agg.index else pd.Series({'mean': np.nan, 'sem': 0})\n",
        "        g_mean = g_row['mean'] if not np.isnan(g_row['mean']) else 0\n",
        "        g_se = g_row['sem'] if not np.isnan(g_row['sem']) else 0\n",
        "        l_mean = l_row['mean'] if not np.isnan(l_row['mean']) else 0\n",
        "        l_se = l_row['sem'] if not np.isnan(l_row['sem']) else 0\n",
        "        ax.bar(x[i] - width/2, g_mean, width, yerr=g_se, label='Graph' if i == 0 else None,\n",
        "               color=graph_color, capsize=4)\n",
        "        ax.bar(x[i] + width/2, l_mean, width, yerr=l_se, label='List' if i == 0 else None,\n",
        "               color=list_color, capsize=4)\n",
        "        g_vals = df[(df['dataset']==d) & (df['interface']=='Graph')]['time_min'].dropna().tolist()\n",
        "        l_vals = df[(df['dataset']==d) & (df['interface']=='List')]['time_min'].dropna().tolist()\n",
        "        pv = pvalue_graph_vs_list(g_vals, l_vals)\n",
        "        timing_sig.append(\"y\" if pv < 0.05 else \"n\")\n",
        "    timing_labels = [f\"{d} ({s})\" for d, s in zip(datasets, timing_sig)]\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(timing_labels)\n",
        "    ax.set_ylabel('Time (min)')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_title('Time per interface × dataset')\n",
        "    ax.legend()\n",
        "    sns.despine(ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Plot 2: Paired bars by interface only (all datasets combined)\n",
        "    agg2 = df.groupby('interface')['time_min'].agg(['mean', 'sem'])\n",
        "    fig2, ax2 = plt.subplots(figsize=(6, 5))\n",
        "    order = ['Graph', 'List']\n",
        "    x2 = np.arange(len(order))\n",
        "    width = 0.6\n",
        "    for i, iface in enumerate(order):\n",
        "        if iface in agg2.index:\n",
        "            mean = agg2.loc[iface, 'mean']\n",
        "            se = agg2.loc[iface, 'sem']\n",
        "            se = 0 if np.isnan(se) else se\n",
        "            color = graph_color if iface == 'Graph' else list_color\n",
        "            ax2.bar(x2[i], mean, width, yerr=se, color=color, capsize=4, label=iface)\n",
        "    g_vals = df[df['interface']=='Graph']['time_min'].dropna().tolist()\n",
        "    l_vals = df[df['interface']=='List']['time_min'].dropna().tolist()\n",
        "    pv_all = pvalue_graph_vs_list(g_vals, l_vals)\n",
        "    sig_all = \"y\" if pv_all < 0.05 else \"n\"\n",
        "    ax2.set_xticks(x2)\n",
        "    ax2.set_xticklabels(order)\n",
        "    ax2.set_ylabel('Time (min)')\n",
        "    ax2.set_xlabel('')\n",
        "    ax2.set_title(f'Time per interface (all datasets) ({sig_all})')\n",
        "    ax2.legend()\n",
        "    sns.despine(ax=ax2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_timing_summary(timing_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Run Timing Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comp_meta = [c for c in comparisons_df.columns if 'Metadata' in c][0]\n",
        "comp_pairs = extract_pid_study_pairs(comparisons_df[comp_meta])\n",
        "valid_pid_study_pairs = {(p, s) for (p, s) in valid_pid_study_pairs | comp_pairs if p in valid_prolific_pids}\n",
        "print(f\"Valid (pid, study_id) pairs: {len(valid_pid_study_pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Histograms by Section (Graph vs List)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Second Task Only: Time and Accuracy\n",
        "\n",
        "Compare time and accuracy for only the **second task** per participant (by Timestamp order)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to second task per (prolific_pid, study_id)\n",
        "sd_task2 = single_dist_filtered.sort_values(['prolific_pid', 'study_id', 'Timestamp']).groupby(['prolific_pid', 'study_id'], as_index=False).nth(1)\n",
        "m_df_task2 = sd_task2[sd_task2[\"What were the questions about\"] == \"Monsters\"]\n",
        "p_df_task2 = sd_task2[sd_task2[\"What were the questions about\"] == \"Locations\"]\n",
        "\n",
        "# Compute accuracy for second task (uses same cols from accuracy cell)\n",
        "monsters_task2 = compute_accuracy(m_df_task2, MONSTERS_COLS, GROUND_TRUTH[\"monsters\"], \"monsters\")\n",
        "places_task2 = compute_accuracy(p_df_task2, PLACES_COLS, GROUND_TRUTH[\"places\"], \"places\")\n",
        "\n",
        "# Build accuracy_df_task2 (same structure as accuracy_df)\n",
        "def mean_n(vals):\n",
        "    clean = [v for v in vals if not (isinstance(v, float) and np.isnan(v))]\n",
        "    return (np.mean(clean), len(clean)) if clean else (np.nan, 0)\n",
        "\n",
        "rows = []\n",
        "for dataset, res in [(\"Monsters\", monsters_task2), (\"Places\", places_task2)]:\n",
        "    all_labels = sorted(set(k.replace(\"_graph\", \"\").replace(\"_list\", \"\") for k in res))\n",
        "    for label in all_labels:\n",
        "        g_m, g_n = mean_n(res.get(label + \"_graph\", []))\n",
        "        l_m, l_n = mean_n(res.get(label + \"_list\", []))\n",
        "        rows.append({\n",
        "            \"Dataset\": dataset,\n",
        "            \"Question\": label.replace(\"_\", \" \").replace(\"bucket \", \"bucket: \"),\n",
        "            \"Graph_mean\": g_m, \"Graph_n\": g_n, \"List_mean\": l_m, \"List_n\": l_n,\n",
        "        })\n",
        "accuracy_df_task2 = pd.DataFrame(rows)\n",
        "\n",
        "# Print second-task accuracy summary\n",
        "print(\"=\" * 80)\n",
        "print(\"SECOND TASK ONLY: Accuracy by Interface (Graph vs List)\")\n",
        "print(\"=\" * 80)\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    sub = accuracy_df_task2[accuracy_df_task2[\"Dataset\"] == dataset]\n",
        "    print(f\"\\n--- {dataset} ---\")\n",
        "    for _, r in sub.iterrows():\n",
        "        g_str = f\"{r['Graph_mean']:.3f} (n={int(r['Graph_n'])})\" if not np.isnan(r['Graph_mean']) else \"-\"\n",
        "        l_str = f\"{r['List_mean']:.3f} (n={int(r['List_n'])})\" if not np.isnan(r['List_mean']) else \"-\"\n",
        "        print(f\"  {str(r['Question'])[:45]:45s}  Graph: {g_str:18}   List: {l_str}\")\n",
        "\n",
        "# Second-task accuracy plots\n",
        "results_map_task2 = {\"Monsters\": monsters_task2, \"Places\": places_task2}\n",
        "plot_rows_t2 = []\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    res = results_map_task2[dataset]\n",
        "    sub = accuracy_df_task2[accuracy_df_task2[\"Dataset\"] == dataset].copy()\n",
        "    bucket_sub = sub[sub[\"Question\"].str.startswith(\"bucket:\")]\n",
        "    exact_sub = sub[~sub[\"Question\"].str.startswith(\"bucket:\")]\n",
        "    for group_name, entity_names in BUCKET_GROUPS[dataset].items():\n",
        "        matches = bucket_sub[bucket_sub[\"Question\"].str.extract(r\"bucket:\\s*(.+)\", expand=False).str.strip().isin(entity_names)]\n",
        "        if len(matches) > 0:\n",
        "            g_m, g_n = wmean_rows(matches, matches, \"Graph\")\n",
        "            l_m, l_n = wmean_rows(matches, matches, \"List\")\n",
        "            g_keys = [f\"bucket_{e}_graph\" for e in entity_names if f\"bucket_{e}_graph\" in res]\n",
        "            l_keys = [f\"bucket_{e}_list\" for e in entity_names if f\"bucket_{e}_list\" in res]\n",
        "            g_scores = np.mean([res[k] for k in g_keys], axis=0) if g_keys else []\n",
        "            l_scores = np.mean([res[k] for k in l_keys], axis=0) if l_keys else []\n",
        "            plot_rows_t2.append({\"Dataset\": dataset, \"Question\": group_name, \"Graph_mean\": g_m, \"Graph_se\": se_from_scores(g_scores), \"Graph_n\": int(g_n), \"List_mean\": l_m, \"List_se\": se_from_scores(l_scores), \"List_n\": int(l_n), \"p_value\": pvalue_graph_vs_list(g_scores, l_scores)})\n",
        "    for _, r in exact_sub.iterrows():\n",
        "        label = question_to_label(r[\"Question\"])\n",
        "        g_scores = res.get(label + \"_graph\", [])\n",
        "        l_scores = res.get(label + \"_list\", [])\n",
        "        plot_rows_t2.append({\"Dataset\": r[\"Dataset\"], \"Question\": r[\"Question\"], \"Graph_mean\": r[\"Graph_mean\"], \"Graph_se\": se_from_scores(g_scores), \"Graph_n\": int(r[\"Graph_n\"]), \"List_mean\": r[\"List_mean\"], \"List_se\": se_from_scores(l_scores), \"List_n\": int(r[\"List_n\"]), \"p_value\": pvalue_graph_vs_list(g_scores, l_scores)})\n",
        "plot_df_task2 = pd.DataFrame(plot_rows_t2)\n",
        "\n",
        "overall_rows_t2 = []\n",
        "for dataset in [\"Monsters\", \"Places\"]:\n",
        "    res = results_map_task2[dataset]\n",
        "    sub = plot_df_task2[plot_df_task2[\"Dataset\"] == dataset]\n",
        "    g_m, g_n = wmean_rows(sub, sub, \"Graph\")\n",
        "    l_m, l_n = wmean_rows(sub, sub, \"List\")\n",
        "    g_keys = [k for k in res if k.endswith(\"_graph\")]\n",
        "    l_keys = [k for k in res if k.endswith(\"_list\")]\n",
        "    g_scores = np.mean([res[k] for k in g_keys], axis=0) if g_keys else []\n",
        "    l_scores = np.mean([res[k] for k in l_keys], axis=0) if l_keys else []\n",
        "    overall_rows_t2.append({\"Dataset\": dataset, \"Question\": \"Overall\", \"Graph_mean\": g_m, \"Graph_se\": se_from_scores(g_scores), \"Graph_n\": int(g_n), \"List_mean\": l_m, \"List_se\": se_from_scores(l_scores), \"List_n\": int(l_n), \"p_value\": pvalue_graph_vs_list(g_scores, l_scores)})\n",
        "overall_df_task2 = pd.DataFrame(overall_rows_t2)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, max(6, len(plot_df_task2) * 0.35)))\n",
        "palette = sns.color_palette(\"muted\")\n",
        "graph_color, list_color = palette[0], palette[1]\n",
        "for ax_idx, dataset in enumerate([\"Monsters\", \"Places\"]):\n",
        "    ax = axes[ax_idx]\n",
        "    sub = plot_df_task2[plot_df_task2[\"Dataset\"] == dataset]\n",
        "    n = len(sub)\n",
        "    y = np.arange(n)\n",
        "    width = 0.35\n",
        "    g_se = sub[\"Graph_se\"].fillna(0).values\n",
        "    l_se = sub[\"List_se\"].fillna(0).values\n",
        "    ax.barh(y - width/2, sub[\"Graph_mean\"], width, xerr=g_se, label=\"Graph\", color=graph_color, edgecolor=\"black\", linewidth=0.5, capsize=2)\n",
        "    ax.barh(y + width/2, sub[\"List_mean\"], width, xerr=l_se, label=\"List\", color=list_color, edgecolor=\"black\", linewidth=0.5, capsize=2)\n",
        "    labels = [f\"{q} (y)\" if row.get(\"p_value\", 1) < 0.05 else f\"{q} (n)\" for q, (_, row) in zip(sub[\"Question\"], sub.iterrows())]\n",
        "    ax.set_yticks(y)\n",
        "    ax.set_yticklabels(labels, fontsize=9)\n",
        "    ax.set_xlim(0, 1.05)\n",
        "    ax.set_xlabel(\"Accuracy\")\n",
        "    ax.set_title(f\"{dataset} – Second Task: Accuracy (Graph vs List)\")\n",
        "    ax.legend(loc=\"lower right\", fontsize=8)\n",
        "    ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
        "plt.suptitle(\"Second Task Only: Accuracy by Question\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "x = np.arange(2)\n",
        "width = 0.35\n",
        "g_se = overall_df_task2[\"Graph_se\"].fillna(0).values\n",
        "l_se = overall_df_task2[\"List_se\"].fillna(0).values\n",
        "ax.bar(x - width/2, overall_df_task2[\"Graph_mean\"], width, yerr=g_se, label=\"Graph\", color=graph_color, edgecolor=\"black\", capsize=4)\n",
        "ax.bar(x + width/2, overall_df_task2[\"List_mean\"], width, yerr=l_se, label=\"List\", color=list_color, edgecolor=\"black\", capsize=4)\n",
        "overall_labels_t2 = [f\"{d} (y)\" if row.get(\"p_value\", 1) < 0.05 else f\"{d} (n)\" for d, (_, row) in zip(overall_df_task2[\"Dataset\"], overall_df_task2.iterrows())]\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(overall_labels_t2)\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_xlabel(\"Dataset\")\n",
        "ax.set_title(\"Second Task Only: Overall Average Accuracy (Graph vs List)\")\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.axhline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
        "for i, row in overall_df_task2.iterrows():\n",
        "    ax.text(i - width/2, row[\"Graph_mean\"] + 0.02, f'{row[\"Graph_mean\"]:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "    ax.text(i + width/2, row[\"List_mean\"] + 0.02, f'{row[\"List_mean\"]:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Second-task timing\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECOND TASK ONLY: Time by Interface and Dataset\")\n",
        "print(\"=\" * 80)\n",
        "if len(timing_task2_df) > 0:\n",
        "    print(timing_task2_df.groupby(['interface', 'dataset']).agg(count=('time_sec', 'count'), mean_sec=('time_sec', 'mean')).round(1))\n",
        "    plot_timing_summary(timing_task2_df)\n",
        "else:\n",
        "    print(\"No second-task timing data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Learning Rates: Interface × Task Order (First vs Second)\n",
        "\n",
        "Single histogram: time and total accuracy by Graph/First, List/First, Graph/Second, List/Second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First task per participant\n",
        "sd_first = single_dist_filtered.sort_values(['prolific_pid', 'study_id', 'Timestamp']).groupby(['prolific_pid', 'study_id'], as_index=False).nth(0)\n",
        "m_df_first = sd_first[sd_first[\"What were the questions about\"] == \"Monsters\"]\n",
        "p_df_first = sd_first[sd_first[\"What were the questions about\"] == \"Locations\"]\n",
        "\n",
        "monsters_first = compute_accuracy(m_df_first, MONSTERS_COLS, GROUND_TRUTH[\"monsters\"], \"monsters\")\n",
        "places_first = compute_accuracy(p_df_first, PLACES_COLS, GROUND_TRUTH[\"places\"], \"places\")\n",
        "\n",
        "def mean_acc_by_interface_order(res_first, res_second, iface_suffix):\n",
        "    \"\"\"Per-participant mean accuracy for an interface across first and second tasks.\"\"\"\n",
        "    keys_first = [k for k in res_first if k.endswith(iface_suffix)]\n",
        "    keys_second = [k for k in res_second if k.endswith(iface_suffix)]\n",
        "    first_scores = np.mean([res_first[k] for k in keys_first], axis=0).tolist() if keys_first else []\n",
        "    second_scores = np.mean([res_second[k] for k in keys_second], axis=0).tolist() if keys_second else []\n",
        "    return first_scores, second_scores\n",
        "\n",
        "g_monsters = mean_acc_by_interface_order(monsters_first, monsters_task2, \"_graph\")\n",
        "g_places = mean_acc_by_interface_order(places_first, places_task2, \"_graph\")\n",
        "l_monsters = mean_acc_by_interface_order(monsters_first, monsters_task2, \"_list\")\n",
        "l_places = mean_acc_by_interface_order(places_first, places_task2, \"_list\")\n",
        "\n",
        "def concat_mean(a, b):\n",
        "    combined = np.concatenate([np.array(a), np.array(b)]) if (a or b) else np.array([])\n",
        "    return np.nanmean(combined) if len(combined) > 0 else np.nan\n",
        "\n",
        "graph_first_acc = concat_mean(g_monsters[0], g_places[0])\n",
        "graph_second_acc = concat_mean(g_monsters[1], g_places[1])\n",
        "list_first_acc = concat_mean(l_monsters[0], l_places[0])\n",
        "list_second_acc = concat_mean(l_monsters[1], l_places[1])\n",
        "\n",
        "timing_first_df = timing_df[timing_df['task_idx'] == 0].copy() if 'task_idx' in timing_df.columns else pd.DataFrame()\n",
        "if len(timing_first_df) > 0:\n",
        "    timing_first_df['time_min'] = timing_first_df['time_sec'] / 60\n",
        "t2_df = timing_task2_df.copy() if len(timing_task2_df) > 0 else pd.DataFrame()\n",
        "if len(t2_df) > 0:\n",
        "    t2_df['time_min'] = t2_df['time_sec'] / 60\n",
        "\n",
        "def safe_mean(ser):\n",
        "    v = ser.dropna()\n",
        "    return v.mean() if len(v) > 0 else np.nan\n",
        "def safe_sem(ser):\n",
        "    v = ser.dropna()\n",
        "    return v.sem() if len(v) > 1 else 0\n",
        "\n",
        "graph_first_time = safe_mean(timing_first_df[timing_first_df['interface']=='Graph']['time_min']) if len(timing_first_df) > 0 else np.nan\n",
        "graph_second_time = safe_mean(t2_df[t2_df['interface']=='Graph']['time_min']) if len(t2_df) > 0 else np.nan\n",
        "list_first_time = safe_mean(timing_first_df[timing_first_df['interface']=='List']['time_min']) if len(timing_first_df) > 0 else np.nan\n",
        "list_second_time = safe_mean(t2_df[t2_df['interface']=='List']['time_min']) if len(t2_df) > 0 else np.nan\n",
        "\n",
        "graph_first_se_t = safe_sem(timing_first_df[timing_first_df['interface']=='Graph']['time_min']) if len(timing_first_df) > 0 else 0\n",
        "graph_second_se_t = safe_sem(t2_df[t2_df['interface']=='Graph']['time_min']) if len(t2_df) > 0 else 0\n",
        "list_first_se_t = safe_sem(timing_first_df[timing_first_df['interface']=='List']['time_min']) if len(timing_first_df) > 0 else 0\n",
        "list_second_se_t = safe_sem(t2_df[t2_df['interface']=='List']['time_min']) if len(t2_df) > 0 else 0\n",
        "\n",
        "conditions = ['Graph / First', 'List / First', 'Graph / Second', 'List / Second']\n",
        "acc_vals = [graph_first_acc, list_first_acc, graph_second_acc, list_second_acc]\n",
        "time_vals = [graph_first_time, list_first_time, graph_second_time, list_second_time]\n",
        "time_ses = [graph_first_se_t, list_first_se_t, graph_second_se_t, list_second_se_t]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "palette = sns.color_palette(\"muted\")\n",
        "colors = [palette[0], palette[1], palette[0], palette[1]]\n",
        "hatches = ['', '', '///', '///']\n",
        "x = np.arange(4)\n",
        "width = 0.6\n",
        "\n",
        "ax1 = axes[0]\n",
        "bars1 = ax1.bar(x, time_vals, width, color=colors, hatch=hatches, edgecolor='black')\n",
        "ax1.errorbar(x, time_vals, yerr=time_ses, fmt='none', color='black', capsize=4)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(conditions, rotation=15, ha='right')\n",
        "ax1.set_ylabel('Time (min)')\n",
        "ax1.set_title('Time by Interface × Task Order')\n",
        "sns.despine(ax=ax1)\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.bar(x, acc_vals, width, color=colors, hatch=hatches, edgecolor='black')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(conditions, rotation=15, ha='right')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Total Accuracy by Interface × Task Order')\n",
        "ax2.set_ylim(0, 1.05)\n",
        "ax2.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "sns.despine(ax=ax2)\n",
        "\n",
        "for i, v in enumerate(acc_vals):\n",
        "    if not np.isnan(v):\n",
        "        ax2.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=palette[0], label='Graph'), Patch(facecolor=palette[1], label='List'),\n",
        "                  Patch(facecolor='white', hatch='///', edgecolor='gray', label='Second task')]\n",
        "fig.legend(handles=legend_elements, loc='upper center', ncol=3, bbox_to_anchor=(0.5, -0.02))\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(bottom=0.18)\n",
        "plt.suptitle('Learning Rates: Time and Accuracy by Interface and Task Order', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print('Time (min):', dict(zip(conditions, [f'{t:.1f}' if not np.isnan(t) else '-' for t in time_vals])))\n",
        "print('Accuracy:', dict(zip(conditions, [f'{a:.3f}' if not np.isnan(a) else '-' for a in acc_vals])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Individual Participants: Time by Task (Scatterplot)\n",
        "\n",
        "Each person: two dots (task 1 and task 2) connected by a line. Color = interface (blue=Graph, orange=List)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-participant timing: need both tasks, build (pid, study_id) -> [(task_idx, time_min, interface), ...]\n",
        "timing_with_pid = timing_df.copy()\n",
        "timing_with_pid['time_min'] = timing_with_pid['time_sec'] / 60\n",
        "timing_with_pid['participant'] = timing_with_pid.apply(lambda r: f\"{r['pid']}_{r['study_id']}\", axis=1)\n",
        "\n",
        "# Only participants with BOTH task 1 and task 2 timing\n",
        "task_counts = timing_with_pid.groupby('participant')['task_idx'].nunique()\n",
        "complete = task_counts[task_counts == 2].index.tolist()\n",
        "t_plot = timing_with_pid[timing_with_pid['participant'].isin(complete)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "palette = sns.color_palette(\"muted\")\n",
        "graph_color, list_color = palette[0], palette[1]\n",
        "\n",
        "for part in t_plot['participant'].unique():\n",
        "    sub = t_plot[t_plot['participant'] == part].sort_values('task_idx')\n",
        "    if len(sub) != 2:\n",
        "        continue\n",
        "    x_vals = sub['task_idx'].values + 1  # 1 and 2 for task 1, task 2\n",
        "    y_vals = sub['time_min'].values\n",
        "    interfaces = sub['interface'].values\n",
        "    line_color = graph_color if interfaces[0] == 'Graph' else list_color\n",
        "    ax.plot(x_vals, y_vals, color=line_color, linestyle='-', linewidth=0.8, alpha=0.6, zorder=0)\n",
        "    for i, (xi, yi, iface) in enumerate(zip(x_vals, y_vals, interfaces)):\n",
        "        color = graph_color if iface == 'Graph' else list_color\n",
        "        ax.scatter(xi, yi, c=[color], marker='o', s=64, zorder=1, alpha=0.6, edgecolors='none')\n",
        "\n",
        "ax.set_xticks([1, 2])\n",
        "ax.set_xticklabels(['Task 1', 'Task 2'])\n",
        "ax.set_xlabel('Task')\n",
        "ax.set_ylabel('Time (min)')\n",
        "ax.set_title('Time by Task per Participant (line connects same person)')\n",
        "from matplotlib.lines import Line2D\n",
        "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=graph_color, markersize=10, alpha=0.6, label='Graph'),\n",
        "                  Line2D([0], [0], marker='o', color='w', markerfacecolor=list_color, markersize=10, alpha=0.6, label='List')]\n",
        "ax.legend(handles=legend_elements)\n",
        "sns.despine(ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Individual Participants: Accuracy by Task (Scatterplot)\n",
        "\n",
        "Same layout as time scatter: two dots per person (task 1 and 2) connected by line; blue=Graph, orange=List."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy_per_row(res, df, task_idx):\n",
        "    \"\"\"Build (pid, study_id, task_idx, interface, accuracy) from compute_accuracy results.\"\"\"\n",
        "    rows = []\n",
        "    for iface in ['graph', 'list']:\n",
        "        subset = df[df['interface'] == iface]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        keys = [k for k in res if k.endswith(f'_{iface}')]\n",
        "        if not keys:\n",
        "            continue\n",
        "        all_scores = np.array([res[k] for k in keys])\n",
        "        mean_acc = np.mean(all_scores, axis=0)\n",
        "        iface_cap = 'Graph' if iface == 'graph' else 'List'\n",
        "        for i, (_, row) in enumerate(subset.iterrows()):\n",
        "            if i < len(mean_acc):\n",
        "                rows.append({'pid': row['prolific_pid'], 'study_id': row['study_id'], 'task_idx': task_idx,\n",
        "                           'interface': iface_cap, 'accuracy': mean_acc[i]})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "acc_first = pd.concat([accuracy_per_row(monsters_first, m_df_first, 0), accuracy_per_row(places_first, p_df_first, 0)], ignore_index=True)\n",
        "acc_second = pd.concat([accuracy_per_row(monsters_task2, m_df_task2, 1), accuracy_per_row(places_task2, p_df_task2, 1)], ignore_index=True)\n",
        "acc_all = pd.concat([acc_first, acc_second], ignore_index=True)\n",
        "acc_all['participant'] = acc_all.apply(lambda r: f\"{r['pid']}_{r['study_id']}\", axis=1)\n",
        "\n",
        "acc_task_counts = acc_all.groupby('participant')['task_idx'].nunique()\n",
        "acc_complete = acc_task_counts[acc_task_counts == 2].index.tolist()\n",
        "a_plot = acc_all[acc_all['participant'].isin(acc_complete)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "palette = sns.color_palette(\"muted\")\n",
        "graph_color, list_color = palette[0], palette[1]\n",
        "\n",
        "for part in a_plot['participant'].unique():\n",
        "    sub = a_plot[a_plot['participant'] == part].sort_values('task_idx')\n",
        "    if len(sub) != 2:\n",
        "        continue\n",
        "    x_vals = sub['task_idx'].values + 1\n",
        "    y_vals = sub['accuracy'].values\n",
        "    interfaces = sub['interface'].values\n",
        "    line_color = graph_color if interfaces[0] == 'Graph' else list_color\n",
        "    ax.plot(x_vals, y_vals, color=line_color, linestyle='-', linewidth=0.8, alpha=0.6, zorder=0)\n",
        "    for xi, yi, iface in zip(x_vals, y_vals, interfaces):\n",
        "        color = graph_color if iface == 'Graph' else list_color\n",
        "        ax.scatter(xi, yi, c=[color], marker='o', s=64, zorder=1, alpha=0.6, edgecolors='none')\n",
        "\n",
        "ax.set_xticks([1, 2])\n",
        "ax.set_xticklabels(['Task 1', 'Task 2'])\n",
        "ax.set_xlabel('Task')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy by Task per Participant (line connects same person)')\n",
        "ax.set_ylim(0, 1.05)\n",
        "from matplotlib.lines import Line2D\n",
        "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=graph_color, markersize=10, alpha=0.6, label='Graph'),\n",
        "                  Line2D([0], [0], marker='o', color='w', markerfacecolor=list_color, markersize=10, alpha=0.6, label='List')]\n",
        "ax.legend(handles=legend_elements)\n",
        "sns.despine(ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "- **Cohort**: Participants with valid prolific_pid in Comparisons → all other data filtered to these PIDs.\n",
        "- **Comparisons**: 5 histograms for 1–7 scale (1=graph, 7=list).\n",
        "- **Single Distribution**: (1) Paired histograms by section for Graph vs List; (2) horizontal stacked bar charts (same data) with diverging palette.\n",
        "- **Timing**: From outputs telemetry, time between consecutive \"next\" navigations on survey pages, by (interface × dataset) and by interface.\n",
        "- **Second Task Only**: Accuracy and timing for chronologically second task per participant; compares Graph vs List on time and correctness.\n",
        "- **Learning Rates**: Single histogram of time and total accuracy by interface × task order (Graph/First, List/First, Graph/Second, List/Second).\n",
        "- **Individual scatterplot**: Time by task per participant; dots connected by line; circle=Graph, x=List."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.9.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
